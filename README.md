# 🦙 RAG with LlamaIndex and Groq

This project demonstrates a simple yet powerful Retrieval-Augmented Generation (RAG) setup using [LlamaIndex](https://www.llamaindex.ai/), [Groq LLM](https://groq.com/), and HuggingFace Embeddings.

> ✅ Notebook ready for Colab with one-click access.

---


## 📓 Chatwithdoc.ipynb

- Install and use LlamaIndex with Groq LLM integration.
- Embed and index documents using HuggingFace embeddings.
- Run conversational RAG using Groq's high-speed inference.

### 🚀 Try on Colab

You can launch the notebook directly on Google Colab using the badge below:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Baishaki-sfdc/RAG-llamaindex-groq/blob/main/Chatwithdoc.ipynb)

---

## 📦 Installation

Install all required dependencies using the following pip command:

```bash
pip install llama-index==0.10.18 \
            llama-index-llms-groq==0.1.3 \
            groq==0.4.2 \
            llama-index-embeddings-huggingface==0.2.0


 Features
🔍 Document loading and indexing

🧠 Embedding via HuggingFace models

⚡ Conversational AI using Groq LLM

📎 RAG (Retrieval-Augmented Generation) for chat



✅ Compatibility
Tested with:

Python 3.11

Google Colab



